
high-level todos:
 - complete dreamer and world-models model learning today



todos:
 - generate samples
 - 



future todos:
 - compare parameters in dreamer and world-models.. should be roughly the same
 - how to do hyperparameter search.. tune?
 - change sampling procedure to same as supervised learning.. sample from shuffled data IDs:
    - training loop can probably done in terms of epochs, as dataset is finite





work on porting dreamer to this repo
shape the apis based on the requirement of dreamer
other models should have similar use-cases as well.


check how things are sampled from danijar dreamer. 
 - sampling is checked. do it in pytorch?
 - replicate it

create a training loop
 - see how dreamer does it

copy relevant files

tf or pytorch?






when to stop the simulation:
 - when you added required number of steps inside buffer (or disk)

for logging, is log_every = 1000 log every 1000 steps:
 - yes, every 1000 steps, we log. 
 - steps are based on num_envs and action_repeat

how is self._step updated inside the __call__ method.
 - self._step += num_envs * action_repeat
    - if num_envs = 1, action_repeat = 2, 2 steps have been added.
    - self._step works in the same way as count_steps -> steps in the main method
 - __call__ is called for each new transition added. each transition is worth action_repeat steps
 - self._step also accounts for those transitions are being generated, but have not yet been saved
    - this is why for ending training on those that are on disk are considered

who is counting the steps, if the graphs being generated are based on no. of steps:
 - counted inside of __call__ method in the form of self._step
 - also inside the main method, count_steps is used to count the steps in buffer (or disk)


does dreamer load episodes into memory or just load them from the disk:
 - 



create the training loop.
 - given any model, get some samples from the episodes and 'update' dummy model
    - train with dreamer style samples
    -    



 - complete the dreamer parts with dreamer-pytorch, can danijar dreamer by used?




 - save episodes how dreamer saves it, but setting up danijar dreamer with wolf




 - saving and loading of models from disk



<current>
 - modifying danijar dreamer to record dtse obs episodes
 <left above>
 - running sowcode3 to record compact dtse episodes. then resume testing the framework.



 - dataset of tf is not working as expected
    - debug it with some print statements inside preprocess
       - issue might not be there as error still persists without preprocess
       - does danijar dreamer give same error if preprocess is commented

    - can use rlpyt framework for handling sequence observations
    - can use pytorch dataset and handle everything myself

 - make sample numpy? yes.



- fix preprocessing








dreamer episode:
>>> episode.keys()
dict_keys(['image', 'action', 'reward', 'discount'])
>>> episode['image'].shape
(501, 64, 64, 3)
>>> episode['action'].shape
(501, 18)
>>> episode['reward'].shape
(501,)
>>> episode['discount'].shape
(501,)

prec.global_policy().compute_dtype
'float32'


{k: v.dtype for k, v in episode.items()}
{'action': dtype('int64'), 'image': dtype('float64'), 'reward': dtype('int64')}